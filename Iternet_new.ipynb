{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "# from tqdm.notebook import tqdm\n",
    "from random import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from glob import glob\n",
    "import sys\n",
    "import shutil  \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainImages = sorted(glob('Cropped MIC dataset/Martian Inverted Channels' + '/*/images/*.jpg'))\n",
    "if not os.path.exists('Cropped MIC dataset/TrainingData'):\n",
    "  os.makedirs('Cropped MIC dataset/TrainingData')\n",
    "if not os.path.exists('Cropped MIC dataset/TrainingData/TrainingPatches'):\n",
    "  os.makedirs('Cropped MIC dataset/TrainingData/TrainingPatches')\n",
    "if not os.path.exists('Cropped MIC dataset/TrainingData/TrainingPatches/images'):\n",
    "  os.makedirs('Cropped MIC dataset/TrainingData/TrainingPatches/images')\n",
    "if not os.path.exists('Cropped MIC dataset/TrainingData/TrainingPatches/masks'):\n",
    "  os.makedirs('Cropped MIC dataset/TrainingData/TrainingPatches/masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testImages = sorted(glob('Cropped MIC dataset/Test data' + '/*/images/*.jpg'))\n",
    "if not os.path.exists('Cropped MIC dataset/TestingData'):\n",
    "  os.makedirs('Cropped MIC dataset/TestingData')\n",
    "if not os.path.exists('Cropped MIC dataset/TestingData/TestingPatches'):\n",
    "  os.makedirs('Cropped MIC dataset/TestingData/TestingPatches')\n",
    "if not os.path.exists('Cropped MIC dataset/TestingData/TestingPatches/images'):\n",
    "  os.makedirs('Cropped MIC dataset/TestingData/TestingPatches/images')\n",
    "if not os.path.exists('Cropped MIC dataset/TestingData/TestingPatches/masks'):\n",
    "  os.makedirs('Cropped MIC dataset/TestingData/TestingPatches/masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "patchSize = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePatches(dirName, images):\n",
    "\n",
    "  patchHeight = patchSize\n",
    "  patchWidth = patchSize\n",
    "  patchList = []\n",
    "\n",
    "  ImageSizes = []\n",
    "  for ImagePath in images:\n",
    "    image = cv2.imread(ImagePath)\n",
    "\n",
    "    mask_path = ImagePath.replace('images', 'masks').replace('.jpg', '.png')\n",
    "    mask = cv2.imread(mask_path)\n",
    "\n",
    "    image_array = np.array(image)\n",
    "    imageSize = image_array.shape\n",
    "    imageHeight = imageSize[0]\n",
    "    imageWidth = imageSize[1]\n",
    "\n",
    "    ImageSizes.append([ImagePath[len(ImagePath)-30:-4],[imageWidth,imageHeight]])\n",
    "\n",
    "\n",
    "    for y in range (0,imageHeight,patchSize):\n",
    "      for x in range (0,imageWidth,patchSize):\n",
    "        if ((y+patchHeight > imageHeight) and (x+patchWidth > imageWidth)):\n",
    "          pat = image [y:imageHeight,x:imageWidth]\n",
    "          patch = np.pad(pat,((0,y+patchHeight-imageHeight),(0,x+patchWidth-imageWidth),(0,0)),'reflect')\n",
    "          maskPatch = mask [y:imageHeight,x:imageWidth]\n",
    "          maskPatch = np.pad(maskPatch,((0,y+patchHeight-imageHeight),(0,x+patchWidth-imageWidth),(0,0)),'reflect')\n",
    "        elif ((y+patchHeight <= imageHeight) and (x+patchWidth > imageWidth)):\n",
    "          pat = image [y:y+patchHeight,x:imageWidth]\n",
    "          patch = np.pad(pat,((0,0),(0,x+patchWidth-imageWidth),(0,0)),'reflect')\n",
    "          maskPatch = mask [y:y+patchHeight,x:imageWidth]\n",
    "          maskPatch = np.pad(maskPatch,((0,0),(0,x+patchWidth-imageWidth),(0,0)),'reflect')\n",
    "        elif ((y+patchHeight > imageHeight) and (x+patchWidth <= imageWidth)):\n",
    "          pat = image [y:imageHeight,x:x+patchWidth]\n",
    "          patch = np.pad(pat,((0,y+patchHeight-imageHeight),(0,0),(0,0)),'reflect')\n",
    "          maskPatch = mask [y:imageHeight,x:x+patchWidth]\n",
    "          maskPatch = np.pad(maskPatch,((0,y+patchHeight-imageHeight),(0,0),(0,0)),'reflect')\n",
    "        else:\n",
    "          patch = image [y:y+patchHeight,x:x+patchWidth]\n",
    "          maskPatch = mask [y:y+patchHeight,x:x+patchWidth]\n",
    "\n",
    "        patchName = ImagePath[len(ImagePath)-30:-4]+\"_\"+str(int(y/patchSize))+\"_\"+str(int(x/patchSize))\n",
    "        cv2.imwrite(dirName+'/images/'+patchName+'.jpg', patch)\n",
    "        cv2.imwrite(dirName+'/masks/'+patchName+'.png', maskPatch)\n",
    "        patchList.append(patch)\n",
    "  return ImageSizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainImages = sorted(glob('Cropped MIC dataset/Martian Inverted Channels' + '/*/images/*.jpg'))\n",
    "testImages = sorted(glob('Cropped MIC dataset/Test data' + '/*/images/*.jpg'))\n",
    "trainImageSizes = makePatches('Cropped MIC dataset/TrainingData/TrainingPatches', trainImages)\n",
    "testImageSizes = makePatches('Cropped MIC dataset/TestingData/TestingPatches', testImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(trainImageSizes))\n",
    "print(len(testImageSizes))\n",
    "print(testImageSizes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import random\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "class segDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, training, transform=None):\n",
    "        super(segDataset, self).__init__()\n",
    "        self.root = root\n",
    "        self.training = training\n",
    "        self.transform = transform\n",
    "        self.IMG_NAMES = sorted(glob(self.root + '/images/*.jpg'))\n",
    "        self.BGR_classes = {'Background' : [ 0, 0, 0],\n",
    "                            'Inverted Channel' : [ 0, 0, 255]} # in BGR\n",
    "\n",
    "        self.bin_classes = ['Background', 'Inverted Channel']\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = self.IMG_NAMES[idx]\n",
    "        mask_path = img_path.replace('images', 'masks').replace('.jpg', '.png')\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        mask = cv2.imread(mask_path)\n",
    "        plt.imshow(mask)\n",
    "        cls_mask = np.zeros(mask.shape)\n",
    "\n",
    "        cls_mask[mask == self.BGR_classes['Background']] = self.bin_classes.index('Background')\n",
    "        cls_mask[mask == self.BGR_classes['Inverted Channel']] = self.bin_classes.index('Inverted Channel')\n",
    "        torch.set_printoptions(profile=\"full\")\n",
    "        #print(\"class mask\",cls_mask)\n",
    "        cls_mask = cls_mask[:,:,2] #removing nearby elements [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -> [0,2,4,6,8]\n",
    "        #print(\"---------------------------------------------------\")\n",
    "        #print(cls_mask)\n",
    "        if self.training==True:\n",
    "            if self.transform:\n",
    "              image = transforms.functional.to_pil_image(image)\n",
    "              image = self.transform(image)\n",
    "              image = np.array(image)\n",
    "\n",
    "        # image = cv2.resize(image, (128,128))/255.0\n",
    "        # cls_mask = cv2.resize(cls_mask, (128,128))\n",
    "        image = cv2.resize(image, (patchSize,patchSize))/255.0\n",
    "        cls_mask = cv2.resize(cls_mask, (patchSize,patchSize)) \n",
    "        image = np.moveaxis(image, -1, 0)\n",
    "\n",
    "        return torch.tensor(image).float(), torch.tensor(cls_mask, dtype=torch.int64), img_path\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.IMG_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDataset = segDataset('Cropped MIC dataset/TestingData', training = False)\n",
    "print(testDataset.IMG_NAMES)\n",
    "len(testDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import random\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "class segTrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, training, transform=None):\n",
    "        super(segTrainDataset, self).__init__()\n",
    "        self.root = root\n",
    "        self.training = training\n",
    "        self.transform = transform\n",
    "        self.IMG_NAMES = sorted(glob(self.root + '/*/images/*.jpg'))\n",
    "        self.BGR_classes = {'Background' : [ 0, 0, 0],\n",
    "                            'Inverted Channel' : [ 0, 0, 255]} # in BGR\n",
    "\n",
    "        self.bin_classes = ['Background', 'Inverted Channel']\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = self.IMG_NAMES[idx]\n",
    "        mask_path = img_path.replace('images', 'masks').replace('.jpg', '.png')\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        mask = cv2.imread(mask_path)\n",
    "        plt.imshow(mask)\n",
    "        cls_mask = np.zeros(mask.shape)\n",
    "\n",
    "        cls_mask[mask == self.BGR_classes['Background']] = self.bin_classes.index('Background')\n",
    "        cls_mask[mask == self.BGR_classes['Inverted Channel']] = self.bin_classes.index('Inverted Channel')\n",
    "        torch.set_printoptions(profile=\"full\")\n",
    "        #print(\"class mask\",cls_mask)\n",
    "        cls_mask = cls_mask[:,:,2] #removing nearby elements [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -> [0,2,4,6,8]\n",
    "        #print(\"---------------------------------------------------\")\n",
    "        #print(cls_mask)\n",
    "        if self.training==True:\n",
    "            if self.transform:\n",
    "              image = transforms.functional.to_pil_image(image)\n",
    "              image = self.transform(image)\n",
    "              image = np.array(image)\n",
    "\n",
    "            # 90 degree rotation\n",
    "            if np.random.rand()<0.5:\n",
    "              angle = np.random.randint(4) * 90\n",
    "              image = ndimage.rotate(image,angle,reshape=True)\n",
    "              cls_mask = ndimage.rotate(cls_mask,angle,reshape=True)\n",
    "\n",
    "        # image = cv2.resize(image, (128,128))/255.0\n",
    "        # cls_mask = cv2.resize(cls_mask, (128,128))\n",
    "        image = cv2.resize(image, (patchSize,patchSize))/255.0\n",
    "        cls_mask = cv2.resize(cls_mask, (patchSize,patchSize)) \n",
    "        image = np.moveaxis(image, -1, 0)\n",
    "\n",
    "        return torch.tensor(image).float(), torch.tensor(cls_mask, dtype=torch.int64), img_path\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.IMG_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1986"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = segTrainDataset('Cropped MIC dataset/TrainingData', training = True)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "250"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDataset = segTrainDataset('Cropped MIC dataset/TestingData', training = True)\n",
    "\n",
    "len(testDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset\n",
    "test_dataset = testDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACH_SIZE = 16\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BACH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleList\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, out_channels=32):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        bilinear = False\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, out_channels)\n",
    "        self.down1 = Down(out_channels, out_channels * 2)\n",
    "        self.down2 = Down(out_channels * 2, out_channels * 4)\n",
    "        self.down3 = Down(out_channels * 4, out_channels * 8)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(out_channels * 8, out_channels * 16 // factor)\n",
    "        self.up1 = Up(out_channels * 16, out_channels * 8 // factor, bilinear)\n",
    "        self.up2 = Up(out_channels * 8, out_channels * 4 // factor, bilinear)\n",
    "        self.up3 = Up(out_channels * 4, out_channels * 2 // factor, bilinear)\n",
    "        self.up4 = Up(out_channels * 2, out_channels, bilinear)\n",
    "        self.outc = OutConv(out_channels, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return x1, x, logits\n",
    "\n",
    "\n",
    "class MiniUNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, out_channels=32):\n",
    "        super(MiniUNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        bilinear = False\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, out_channels)\n",
    "        self.down1 = Down(out_channels, out_channels*2)\n",
    "        self.down2 = Down(out_channels*2, out_channels*4)\n",
    "        self.down3 = Down(out_channels*4, out_channels*8)\n",
    "        self.up1 = Up(out_channels*8, out_channels*4, bilinear)\n",
    "        self.up2 = Up(out_channels*4, out_channels*2, bilinear)\n",
    "        self.up3 = Up(out_channels*2, out_channels, bilinear)\n",
    "        self.outc = OutConv(out_channels, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x = self.up1(x4, x3)\n",
    "        x = self.up2(x, x2)\n",
    "        x = self.up3(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return x1, x, logits\n",
    "\n",
    "\n",
    "class Iternet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, out_channels=32, iterations=3):\n",
    "        super(Iternet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.iterations = iterations\n",
    "\n",
    "        # define the network UNet layer\n",
    "        self.model_unet = UNet(n_channels=n_channels,\n",
    "                               n_classes=n_classes, out_channels=out_channels)\n",
    "\n",
    "        # define the network MiniUNet layers\n",
    "        self.model_miniunet = ModuleList(MiniUNet(\n",
    "            n_channels=out_channels*2, n_classes=n_classes, out_channels=out_channels) for i in range(iterations))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, logits = self.model_unet(x)\n",
    "        for i in range(self.iterations):\n",
    "            x = torch.cat([x1, x2], dim=1)\n",
    "            _, x2, logits = self.model_miniunet[i](x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated\n",
    "class WeightedDiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True, n_classes=2):\n",
    "        super(WeightedDiceLoss, self).__init__()\n",
    "        self.classes = n_classes\n",
    "\n",
    "    def to_one_hot(self, tensor):\n",
    "        n,h,w = tensor.size()\n",
    "        one_hot = torch.zeros(n,self.classes,h,w).to(tensor.device).scatter_(1,tensor.view(n,1,h,w),1)\n",
    "        return one_hot\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "       \n",
    "        N = inputs.size()[0]\n",
    "       \n",
    "\n",
    "        # predicted probabilities for each pixel along channel\n",
    "        inputs = F.softmax(inputs,dim=1)\n",
    "        \n",
    "        \n",
    "        # Numerator Product\n",
    "        target_oneHot = self.to_one_hot(target)\n",
    "        weight_invertedChannels = 3\n",
    "        weight_background = 1\n",
    "\n",
    "        inter = 2*inputs * target_oneHot\n",
    "        #print(\"inter size\", inter.size())\n",
    "        #(2.*intersection + smooth)/(inputs.sum() + target.sum() + smooth)\n",
    "        intersection_channels = 0\n",
    "        intersection_background = 0\n",
    "        # smooth = 1\n",
    "\n",
    "        for imageInd in range(N): \n",
    "          intersection_channels += inter[imageInd][1].view(-1).sum() \n",
    "          intersection_background += inter[imageInd][0].view(-1).sum() \n",
    "\n",
    "        # inter = intersection \n",
    "        #print(\"inter \", inter)\n",
    "        #Denominator \n",
    "        union= inputs + target_oneHot\n",
    "        ## Sum over all pixels N x C x H x W => N x C\n",
    "        union_channels=0\n",
    "        union_background = 0\n",
    "        total2 = 0\n",
    "        for imageInd in range(N):\n",
    "          union_channels += union[imageInd][1].view(-1).sum() \n",
    "          union_background += union[imageInd][0].view(-1).sum() \n",
    "\n",
    "        # union = total2\n",
    "        #print(\"union \", union)\n",
    "        #union = union.view(N,self.classes,-1).sum(2)\n",
    "        dice_channels =intersection_channels/union_channels\n",
    "        dice_channels = dice_channels.mean()*0.8\n",
    "\n",
    "        dice_background =intersection_background/union_background\n",
    "        dice_background = dice_background.mean()*0.2\n",
    "\n",
    "        weighted_dice = (dice_channels+ dice_background)\n",
    "\n",
    "        loss = 1 - weighted_dice\n",
    "        #print(\"loss \", loss)\n",
    "        #print(\"loss mean \", loss.mean())\n",
    "        ## Return average loss over classes and batch\n",
    "        # return 1-loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = WeightedDiceLoss(n_classes=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc(label, predicted):\n",
    "  seg_acc = (y.cpu() == torch.argmax(pred_mask, axis=1).cpu()).sum() / torch.numel(y.cpu())\n",
    "  return seg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_loss = torch.tensor(float('inf'))\n",
    "# model = UNet(n_channels=3, n_classes=2).to(device)\n",
    "model = Iternet(n_channels=3, n_classes=2).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "os.makedirs('./saved_models', exist_ok=True)\n",
    "\n",
    "N_EPOCHS = 30\n",
    "N_DATA = len(train_dataset)\n",
    "N_TEST = len(test_dataset)\n",
    "\n",
    "plot_losses = []\n",
    "scheduler_counter = 0\n",
    "prediction = []\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "  # training\n",
    "  model.train()\n",
    "  loss_list = []\n",
    "  acc_list = []\n",
    "  for batch_i, (x, y,z) in enumerate(train_dataloader):\n",
    "\n",
    "      pred_mask = model(x.to(device))\n",
    "\n",
    "      loss = criterion(pred_mask, y.to(device))\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      loss_list.append(loss.cpu().detach().numpy())\n",
    "      acc_list.append(acc(y,pred_mask).numpy())\n",
    "\n",
    "      sys.stdout.write(\n",
    "          \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f (%f)]\"\n",
    "          % (\n",
    "              epoch,\n",
    "              N_EPOCHS,\n",
    "              batch_i,\n",
    "              len(train_dataloader),\n",
    "              loss.cpu().detach().numpy(),\n",
    "              np.mean(loss_list),\n",
    "          )\n",
    "      )\n",
    "  scheduler_counter += 1\n",
    "  # testing\n",
    "  model.eval()\n",
    "  val_loss_list = []\n",
    "  val_acc_list = []\n",
    "  for batch_i, (x, y, z) in enumerate(test_dataloader):\n",
    "      with torch.no_grad():    \n",
    "          pred_mask = model(x.to(device))\n",
    "      val_loss = criterion(pred_mask, y.to(device))\n",
    "      val_loss_list.append(val_loss.cpu().detach().numpy())\n",
    "      val_acc_list.append(acc(y,pred_mask).numpy())\n",
    "    \n",
    "  print(' epoch {} - loss : {:.5f} - acc : {:.2f} - val loss : {:.5f} - val acc : {:.2f}'.format(epoch, \n",
    "                                                                                                 np.mean(loss_list), \n",
    "                                                                                                 np.mean(acc_list), \n",
    "                                                                                                 np.mean(val_loss_list),\n",
    "                                                                                                 np.mean(val_acc_list)))\n",
    "  plot_losses.append([epoch, np.mean(loss_list), np.mean(val_loss_list)])\n",
    "  #  print(\"prediction \", prediction.size(), prediction.data[0])\n",
    "  compare_loss = np.mean(val_loss_list)\n",
    "  is_best = compare_loss < min_loss\n",
    "  if is_best == True:\n",
    "    scheduler_counter = 0\n",
    "    min_loss = min(compare_loss, min_loss)\n",
    "  torch.save(model.state_dict(), './saved_models/unet_epoch_{}_{:.5f}.pt'.format(epoch,np.mean(val_loss_list)))\n",
    "  \n",
    "  if scheduler_counter > 5:\n",
    "    lr_scheduler.step()\n",
    "    print(f\"lowering learning rate to {optimizer.param_groups[0]['lr']}\")\n",
    "    scheduler_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
