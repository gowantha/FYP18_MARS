{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iBDD6-5Kr5LV"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "from random import shuffle\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from glob import glob\n",
    "import sys\n",
    "import shutil  \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HwhF3kwDsCoC",
    "outputId": "cc117396-4c20-429b-f9d1-476239d68a7a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
      "Collecting gdown\n",
      "  Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.8.2)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
      "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.25.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.12.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
      "Installing collected packages: gdown\n",
      "  Attempting uninstall: gdown\n",
      "    Found existing installation: gdown 4.4.0\n",
      "    Uninstalling gdown-4.4.0:\n",
      "      Successfully uninstalled gdown-4.4.0\n",
      "Successfully installed gdown-4.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --no-cache-dir gdown\n",
    "import gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fOtelO_psJow",
    "outputId": "8c614c8b-d805-41d3-c49a-66feeca8f4f9"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/content/Test data/Aeolis Dorsa/images/ESP_072116_1740_RED.browse.jpg',\n",
       " '/content/Test data/Aeolis Dorsa/masks/ESP_072116_1740_RED.browse.png',\n",
       " '/content/Test data/Miyamoto Crater/images/ESP_016631_1770_RED.browse.jpg',\n",
       " '/content/Test data/Miyamoto Crater/images/ESP_074759_1855_RED.browse.jpg',\n",
       " '/content/Test data/Miyamoto Crater/masks/ESP_016631_1770_RED.browse.png',\n",
       " '/content/Test data/Miyamoto Crater/masks/ESP_074759_1855_RED.browse.png']"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Download training data\n",
    "url = \"https://drive.google.com/drive/folders/16ZaNLVBZgn7RYFLiF1jt-WcCF1aRb8aP\"\n",
    "gdown.download_folder(url, quiet=True, use_cookies=False, remaining_ok=True)\n",
    "\n",
    "# Download test data\n",
    "url = \"https://drive.google.com/drive/folders/1dGwTm47UJSp0_qLl0n0VCUPMCu56xEZD\"\n",
    "gdown.download_folder(url, quiet=True, use_cookies=False, remaining_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "url = \"https://drive.google.com/drive/u/0/folders/1I1vxbZxmgR3OOPE7e-odfpcuCmJElICl\"\n",
    "gdown.download_folder(url, quiet=True, use_cookies=False, remaining_ok=True)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aOvy-9QwMpyS",
    "outputId": "12c4251f-1b7c-454e-9b5f-df8c48180b02"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/content/MIC Classifier/cnn_mic_classifier_20_0.31479.pth',\n",
       " '/content/MIC Classifier/cnn.pth',\n",
       " '/content/MIC Classifier/segmentation_model.pt']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FcAEOQBkXUwf"
   },
   "outputs": [],
   "source": [
    "trainImages = sorted(glob('./Martian Inverted Channels' + '/*/images/*.jpg'))\n",
    "if not os.path.exists('./TrainingData'):\n",
    "  os.makedirs('./TrainingData')\n",
    "if not os.path.exists('./TrainingData/TrainingPatches'):\n",
    "  os.makedirs('./TrainingData/TrainingPatches')\n",
    "if not os.path.exists('./TrainingData/TrainingPatches/images'):\n",
    "  os.makedirs('./TrainingData/TrainingPatches/images')\n",
    "if not os.path.exists('./TrainingData/TrainingPatches/masks'):\n",
    "  os.makedirs('./TrainingData/TrainingPatches/masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lV3Lc9lgXSTK"
   },
   "outputs": [],
   "source": [
    "testImages = sorted(glob('./Test data' + '/*/images/*.jpg'))\n",
    "if not os.path.exists('./TestingData'):\n",
    "  os.makedirs('./TestingData')\n",
    "if not os.path.exists('./TestingData/TestingPatches'):\n",
    "  os.makedirs('./TestingData/TestingPatches')\n",
    "if not os.path.exists('./TestingData/TestingPatches/images'):\n",
    "  os.makedirs('./TestingData/TestingPatches/images')\n",
    "if not os.path.exists('./TestingData/TestingPatches/masks'):\n",
    "  os.makedirs('./TestingData/TestingPatches/masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3CCacGoXm86"
   },
   "outputs": [],
   "source": [
    "patchSize = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeI9ojSyXvNL"
   },
   "outputs": [],
   "source": [
    "def makePatches(dirName, images):\n",
    "\n",
    "  patchHeight = patchSize\n",
    "  patchWidth = patchSize\n",
    "  patchList = []\n",
    "\n",
    "  ImageSizes = []\n",
    "  for ImagePath in images:\n",
    "    image = cv2.imread(ImagePath)\n",
    "\n",
    "    mask_path = ImagePath.replace('images', 'masks').replace('.jpg', '.png')\n",
    "    mask = cv2.imread(mask_path)\n",
    "\n",
    "    image_array = np.array(image)\n",
    "    imageSize = image_array.shape\n",
    "    imageHeight = imageSize[0]\n",
    "    imageWidth = imageSize[1]\n",
    "\n",
    "    ImageSizes.append([ImagePath[len(ImagePath)-30:-4],[imageWidth,imageHeight]])\n",
    "\n",
    "\n",
    "    for y in range (0,imageHeight,patchSize):\n",
    "      for x in range (0,imageWidth,patchSize):\n",
    "        if ((y+patchHeight > imageHeight) and (x+patchWidth > imageWidth)):\n",
    "          pat = image [y:imageHeight,x:imageWidth]\n",
    "          patch = np.pad(pat,((0,y+patchHeight-imageHeight),(0,x+patchWidth-imageWidth),(0,0)),'reflect')\n",
    "          maskPatch = mask [y:imageHeight,x:imageWidth]\n",
    "          maskPatch = np.pad(maskPatch,((0,y+patchHeight-imageHeight),(0,x+patchWidth-imageWidth),(0,0)),'reflect')\n",
    "        elif ((y+patchHeight <= imageHeight) and (x+patchWidth > imageWidth)):\n",
    "          pat = image [y:y+patchHeight,x:imageWidth]\n",
    "          patch = np.pad(pat,((0,0),(0,x+patchWidth-imageWidth),(0,0)),'reflect')\n",
    "          maskPatch = mask [y:y+patchHeight,x:imageWidth]\n",
    "          maskPatch = np.pad(maskPatch,((0,0),(0,x+patchWidth-imageWidth),(0,0)),'reflect')\n",
    "        elif ((y+patchHeight > imageHeight) and (x+patchWidth <= imageWidth)):\n",
    "          pat = image [y:imageHeight,x:x+patchWidth]\n",
    "          patch = np.pad(pat,((0,y+patchHeight-imageHeight),(0,0),(0,0)),'reflect')\n",
    "          maskPatch = mask [y:imageHeight,x:x+patchWidth]\n",
    "          maskPatch = np.pad(maskPatch,((0,y+patchHeight-imageHeight),(0,0),(0,0)),'reflect')\n",
    "        else:\n",
    "          patch = image [y:y+patchHeight,x:x+patchWidth]\n",
    "          maskPatch = mask [y:y+patchHeight,x:x+patchWidth]\n",
    "\n",
    "        patchName = ImagePath[len(ImagePath)-30:-4]+\"_\"+str(int(y/patchSize))+\"_\"+str(int(x/patchSize))\n",
    "        cv2.imwrite(dirName+'/images/'+patchName+'.jpg', patch)\n",
    "        cv2.imwrite(dirName+'/masks/'+patchName+'.png', maskPatch)\n",
    "        patchList.append(patch)\n",
    "  return ImageSizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zPQt8ToYHl_"
   },
   "outputs": [],
   "source": [
    "trainImages = sorted(glob('./Martian Inverted Channels' + '/*/images/*.jpg'))\n",
    "testImages = sorted(glob('./Test data' + '/*/images/*.jpg'))\n",
    "trainImageSizes = makePatches('./TrainingData/TrainingPatches', trainImages)\n",
    "testImageSizes = makePatches('./TestingData/TestingPatches', testImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "igikRc-vIOCM",
    "outputId": "407a9c3e-3247-4062-c731-cb14757f8b6c"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "20\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(trainImageSizes))\n",
    "print(len(testImageSizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRTSH0CZ87aF",
    "outputId": "4971c4f5-9d28-4eae-fd49-e44f489df5e9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['ESP_072116_1740_RED.browse', [1703, 2412]], ['ESP_016631_1770_RED.browse', [1639, 2863]], ['ESP_074759_1855_RED.browse', [1519, 3926]]]\n"
     ]
    }
   ],
   "source": [
    "print(testImageSizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxUDChB6Ydb8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import random\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "class segDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, training, transform=None):\n",
    "        super(segDataset, self).__init__()\n",
    "        self.root = root\n",
    "        self.training = training\n",
    "        self.transform = transform\n",
    "        self.IMG_NAMES = sorted(glob(self.root + '/images/*.jpg'))\n",
    "        self.BGR_classes = {'Background' : [ 0, 0, 0],\n",
    "                            'Inverted Channel' : [ 0, 0, 255]} # in BGR\n",
    "\n",
    "        self.bin_classes = ['Background', 'Inverted Channel']\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = self.IMG_NAMES[idx]\n",
    "        mask_path = img_path.replace('images', 'masks').replace('.jpg', '.png')\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        mask = cv2.imread(mask_path)\n",
    "        plt.imshow(mask)\n",
    "        cls_mask = np.zeros(mask.shape)\n",
    "\n",
    "        cls_mask[mask == self.BGR_classes['Background']] = self.bin_classes.index('Background')\n",
    "        cls_mask[mask == self.BGR_classes['Inverted Channel']] = self.bin_classes.index('Inverted Channel')\n",
    "        torch.set_printoptions(profile=\"full\")\n",
    "        #print(\"class mask\",cls_mask)\n",
    "        cls_mask = cls_mask[:,:,2] #removing nearby elements [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -> [0,2,4,6,8]\n",
    "        #print(\"---------------------------------------------------\")\n",
    "        #print(cls_mask)\n",
    "        if self.training==True:\n",
    "            if self.transform:\n",
    "              image = transforms.functional.to_pil_image(image)\n",
    "              image = self.transform(image)\n",
    "              image = np.array(image)\n",
    "\n",
    "        # image = cv2.resize(image, (128,128))/255.0\n",
    "        # cls_mask = cv2.resize(cls_mask, (128,128))\n",
    "        image = cv2.resize(image, (patchSize,patchSize))/255.0\n",
    "        cls_mask = cv2.resize(cls_mask, (patchSize,patchSize)) \n",
    "        image = np.moveaxis(image, -1, 0)\n",
    "\n",
    "        return torch.tensor(image).float(), torch.tensor(cls_mask, dtype=torch.int64), img_path\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.IMG_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kjzo88WZYfvq",
    "outputId": "1aa3b00c-8696-42b7-9d01-4b06fa27fff9"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "testDataset = segDataset('./TestingData', training = False)\n",
    "print(testDataset.IMG_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gJtJKtFYdn4F"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import random\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "class segTrainDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, training, transform=None):\n",
    "        super(segTrainDataset, self).__init__()\n",
    "        self.root = root\n",
    "        self.training = training\n",
    "        self.transform = transform\n",
    "        self.IMG_NAMES = sorted(glob(self.root + '/*/images/*.jpg'))\n",
    "        self.BGR_classes = {'Background' : [ 0, 0, 0],\n",
    "                            'Inverted Channel' : [ 0, 0, 255]} # in BGR\n",
    "\n",
    "        self.bin_classes = ['Background', 'Inverted Channel']\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_path = self.IMG_NAMES[idx]\n",
    "        mask_path = img_path.replace('images', 'masks').replace('.jpg', '.png')\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        mask = cv2.imread(mask_path)\n",
    "        plt.imshow(mask)\n",
    "        cls_mask = np.zeros(mask.shape)\n",
    "\n",
    "        cls_mask[mask == self.BGR_classes['Background']] = self.bin_classes.index('Background')\n",
    "        cls_mask[mask == self.BGR_classes['Inverted Channel']] = self.bin_classes.index('Inverted Channel')\n",
    "        torch.set_printoptions(profile=\"full\")\n",
    "        #print(\"class mask\",cls_mask)\n",
    "        cls_mask = cls_mask[:,:,2] #removing nearby elements [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] -> [0,2,4,6,8]\n",
    "        #print(\"---------------------------------------------------\")\n",
    "        #print(cls_mask)\n",
    "        if self.training==True:\n",
    "            if self.transform:\n",
    "              image = transforms.functional.to_pil_image(image)\n",
    "              image = self.transform(image)\n",
    "              image = np.array(image)\n",
    "        # image = cv2.resize(image, (128,128))/255.0\n",
    "        # cls_mask = cv2.resize(cls_mask, (128,128))\n",
    "        image = cv2.resize(image, (patchSize,patchSize))/255.0\n",
    "        cls_mask = cv2.resize(cls_mask, (patchSize,patchSize)) \n",
    "        image = np.moveaxis(image, -1, 0)\n",
    "\n",
    "        return torch.tensor(image).float(), torch.tensor(cls_mask, dtype=torch.int64), img_path\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.IMG_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "28-mClendp1x",
    "outputId": "ecc000ef-a701-4ad4-aff0-1d6e2c61af4a"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1986"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "dataset = segTrainDataset('./TrainingData', training = True)\n",
    "\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f_apasYlnNSU",
    "outputId": "3d994504-2305-4b7f-b398-2fb4b5f87689"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "testDataset = segTrainDataset('./TestingData', training = True)\n",
    "\n",
    "len(testDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KupGWOS-egfT"
   },
   "outputs": [],
   "source": [
    "train_dataset = dataset\n",
    "test_dataset = testDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tBLX5nNBekh2"
   },
   "outputs": [],
   "source": [
    "BACH_SIZE = 2\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BACH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sWKLzcETJhCj",
    "outputId": "a92faeef-d2b8-4f47-ede2-c2afe169ab15"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0\n",
      "----------------------------------------------------------\n",
      "2\n",
      "----------------------------------------------------------\n",
      "2\n",
      "----------------------------------------------------------\n",
      "('./TrainingData/TrainingPatches/images/ESP_049346_1845_RED.browse_10_0.jpg', './TrainingData/TrainingPatches/images/ESP_048233_1770_RED.browse_11_1.jpg')\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for batch_i, (x, y,z) in enumerate(train_dataloader):\n",
    "  print(batch_i)\n",
    "  print(\"----------------------------------------------------------\")\n",
    "  print(len(x))\n",
    "  print(\"----------------------------------------------------------\")\n",
    "  print(len(y))\n",
    "  print(\"----------------------------------------------------------\")\n",
    "  print(z)\n",
    "  break\n",
    "  count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eVKtaEchIhmA"
   },
   "outputs": [],
   "source": [
    "BACH_SIZE = 16\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BACH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=1, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-U6bgsajenUW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xe4YPSeTepfo"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.nn import ModuleList\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, out_channels=32):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        bilinear = False\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, out_channels)\n",
    "        self.down1 = Down(out_channels, out_channels * 2)\n",
    "        self.down2 = Down(out_channels * 2, out_channels * 4)\n",
    "        self.down3 = Down(out_channels * 4, out_channels * 8)\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = Down(out_channels * 8, out_channels * 16 // factor)\n",
    "        self.up1 = Up(out_channels * 16, out_channels * 8 // factor, bilinear)\n",
    "        self.up2 = Up(out_channels * 8, out_channels * 4 // factor, bilinear)\n",
    "        self.up3 = Up(out_channels * 4, out_channels * 2 // factor, bilinear)\n",
    "        self.up4 = Up(out_channels * 2, out_channels, bilinear)\n",
    "        self.outc = OutConv(out_channels, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return x1, x, logits\n",
    "\n",
    "\n",
    "class MiniUNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, out_channels=32):\n",
    "        super(MiniUNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        bilinear = False\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, out_channels)\n",
    "        self.down1 = Down(out_channels, out_channels*2)\n",
    "        self.down2 = Down(out_channels*2, out_channels*4)\n",
    "        self.down3 = Down(out_channels*4, out_channels*8)\n",
    "        self.up1 = Up(out_channels*8, out_channels*4, bilinear)\n",
    "        self.up2 = Up(out_channels*4, out_channels*2, bilinear)\n",
    "        self.up3 = Up(out_channels*2, out_channels, bilinear)\n",
    "        self.outc = OutConv(out_channels, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x = self.up1(x4, x3)\n",
    "        x = self.up2(x, x2)\n",
    "        x = self.up3(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return x1, x, logits\n",
    "\n",
    "\n",
    "class Iternet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, out_channels=32, iterations=3):\n",
    "        super(Iternet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.iterations = iterations\n",
    "\n",
    "        # define the network UNet layer\n",
    "        self.model_unet = UNet(n_channels=n_channels,\n",
    "                               n_classes=n_classes, out_channels=out_channels)\n",
    "\n",
    "        # define the network MiniUNet layers\n",
    "        self.model_miniunet = ModuleList(MiniUNet(\n",
    "            n_channels=out_channels*2, n_classes=n_classes, out_channels=out_channels) for i in range(iterations))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1, x2, logits = self.model_unet(x)\n",
    "        for i in range(self.iterations):\n",
    "            x = torch.cat([x1, x2], dim=1)\n",
    "            _, x2, logits = self.model_miniunet[i](x)\n",
    "\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFP1ucH1e4ah"
   },
   "outputs": [],
   "source": [
    "# Updated\n",
    "class WeightedDiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True, n_classes=2):\n",
    "        super(WeightedDiceLoss, self).__init__()\n",
    "        self.classes = n_classes\n",
    "\n",
    "    def to_one_hot(self, tensor):\n",
    "        n,h,w = tensor.size()\n",
    "        one_hot = torch.zeros(n,self.classes,h,w).to(tensor.device).scatter_(1,tensor.view(n,1,h,w),1)\n",
    "        return one_hot\n",
    "\n",
    "    def forward(self, inputs, target):\n",
    "       \n",
    "        N = inputs.size()[0]\n",
    "       \n",
    "\n",
    "        # predicted probabilities for each pixel along channel\n",
    "        inputs = F.softmax(inputs,dim=1)\n",
    "        \n",
    "        \n",
    "        # Numerator Product\n",
    "        target_oneHot = self.to_one_hot(target)\n",
    "        weight_invertedChannels = 3\n",
    "        weight_background = 1\n",
    "\n",
    "        inter = 2*inputs * target_oneHot\n",
    "        #print(\"inter size\", inter.size())\n",
    "        #(2.*intersection + smooth)/(inputs.sum() + target.sum() + smooth)\n",
    "        intersection_channels = 0\n",
    "        intersection_background = 0\n",
    "        # smooth = 1\n",
    "\n",
    "        for imageInd in range(N): \n",
    "          intersection_channels += inter[imageInd][1].view(-1).sum() \n",
    "          intersection_background += inter[imageInd][0].view(-1).sum() \n",
    "\n",
    "        # inter = intersection \n",
    "        #print(\"inter \", inter)\n",
    "        #Denominator \n",
    "        union= inputs + target_oneHot\n",
    "        ## Sum over all pixels N x C x H x W => N x C\n",
    "        union_channels=0\n",
    "        union_background = 0\n",
    "        total2 = 0\n",
    "        for imageInd in range(N):\n",
    "          union_channels += union[imageInd][1].view(-1).sum() \n",
    "          union_background += union[imageInd][0].view(-1).sum() \n",
    "\n",
    "        # union = total2\n",
    "        #print(\"union \", union)\n",
    "        #union = union.view(N,self.classes,-1).sum(2)\n",
    "        dice_channels =intersection_channels/union_channels\n",
    "        dice_channels = dice_channels.mean()*0.8\n",
    "\n",
    "        dice_background =intersection_background/union_background\n",
    "        dice_background = dice_background.mean()*0.2\n",
    "\n",
    "        weighted_dice = (dice_channels+ dice_background)\n",
    "\n",
    "        loss = 1 - weighted_dice\n",
    "        #print(\"loss \", loss)\n",
    "        #print(\"loss mean \", loss.mean())\n",
    "        ## Return average loss over classes and batch\n",
    "        # return 1-loss.mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kcEAJEVOe-sT"
   },
   "outputs": [],
   "source": [
    "criterion = WeightedDiceLoss(n_classes=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pksGqK5pfKgI"
   },
   "outputs": [],
   "source": [
    "def acc(label, predicted):\n",
    "  seg_acc = (y.cpu() == torch.argmax(pred_mask, axis=1).cpu()).sum() / torch.numel(y.cpu())\n",
    "  return seg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHutpdS5fMXO"
   },
   "outputs": [],
   "source": [
    "min_loss = torch.tensor(float('inf'))\n",
    "# model = UNet(n_channels=3, n_classes=2).to(device)\n",
    "model = Iternet(n_channels=3, n_classes=2).to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkXC6bx-FvVn"
   },
   "outputs": [],
   "source": [
    "def getImageIndex(Array, imageID):\n",
    "  imageIndex = -1\n",
    "  for n in range (0,len(Array)):\n",
    "    if Array[n][0] == imageID:\n",
    "      imageIndex = n\n",
    "      break\n",
    "  \n",
    "  return imageIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8_Kx769Fxx3"
   },
   "outputs": [],
   "source": [
    "def getImageSize(imageList, imageID):\n",
    "  imageSize =[patchSize,patchSize]\n",
    "  # imageSize =[128,128]\n",
    "  for image in imageList:\n",
    "    if image[0]==imageID:\n",
    "      imageSize= image[1]\n",
    "      break\n",
    "  return imageSize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kF5h6smifOzW"
   },
   "outputs": [],
   "source": [
    "# from pprint import pprint\n",
    "# os.makedirs('./saved_models', exist_ok=True)\n",
    "\n",
    "# N_EPOCHS = 1\n",
    "# N_DATA = len(train_dataset)\n",
    "# N_TEST = len(test_dataset)\n",
    "\n",
    "# plot_losses = []\n",
    "# scheduler_counter = 0\n",
    "# prediction = []\n",
    "\n",
    "# for epoch in range(N_EPOCHS):\n",
    "#   # training\n",
    "#   model.train()\n",
    "#   loss_list = []\n",
    "#   acc_list = []\n",
    "#   predictions = []\n",
    "#   for batch_i, (x, y, z) in enumerate(train_dataloader):\n",
    "\n",
    "#       pred_mask = model(x.to(device))\n",
    "\n",
    "#       loss = criterion(pred_mask, y.to(device))\n",
    "\n",
    "#       optimizer.zero_grad()\n",
    "#       loss.backward()\n",
    "#       optimizer.step()\n",
    "#       loss_list.append(loss.cpu().detach().numpy())\n",
    "#       acc_list.append(acc(y,pred_mask).numpy())\n",
    "\n",
    "#       sys.stdout.write(\n",
    "#           \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f (%f)]\"\n",
    "#           % (\n",
    "#               epoch,\n",
    "#               N_EPOCHS,\n",
    "#               batch_i,\n",
    "#               len(train_dataloader),\n",
    "#               loss.cpu().detach().numpy(),\n",
    "#               np.mean(loss_list),\n",
    "#           )\n",
    "#       )\n",
    "#   scheduler_counter += 1\n",
    "#   # testing\n",
    "#   model.eval()\n",
    "#   val_loss_list = []\n",
    "#   val_acc_list = []\n",
    "#   for batch_i, (x, y, z) in enumerate(test_dataloader):\n",
    "#       with torch.no_grad():    \n",
    "#           pred_mask = model(x.to(device))\n",
    "#       val_loss = criterion(pred_mask, y.to(device))\n",
    "#       val_loss_list.append(val_loss.cpu().detach().numpy())\n",
    "#       val_acc_list.append(acc(y,pred_mask).numpy())\n",
    "    \n",
    "#   print(' epoch {} - loss : {:.5f} - acc : {:.2f} - val loss : {:.5f} - val acc : {:.2f}'.format(epoch, \n",
    "#                                                                                                  np.mean(loss_list), \n",
    "#                                                                                                  np.mean(acc_list), \n",
    "#                                                                                                  np.mean(val_loss_list),\n",
    "#                                                                                                  np.mean(val_acc_list)))\n",
    "#   plot_losses.append([epoch, np.mean(loss_list), np.mean(val_loss_list)])\n",
    "#   #  print(\"prediction \", prediction.size(), prediction.data[0])\n",
    "#   compare_loss = np.mean(val_loss_list)\n",
    "#   is_best = compare_loss < min_loss\n",
    "#   if is_best == True:\n",
    "#     scheduler_counter = 0\n",
    "#     min_loss = min(compare_loss, min_loss)\n",
    "#   torch.save(model.state_dict(), './saved_models/unet_epoch_{}_{:.5f}.pt'.format(epoch,np.mean(val_loss_list)))\n",
    "  \n",
    "#   if scheduler_counter > 5:\n",
    "#     lr_scheduler.step()\n",
    "#     print(f\"lowering learning rate to {optimizer.param_groups[0]['lr']}\")\n",
    "#     scheduler_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYg7KbzRMDs6",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0269aa6b-90a1-4e0f-df37-6ddccab2c3b3"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Epoch 0/2] [Batch 124/125] [Loss: 0.800507 (0.792080)] epoch 0 - loss : 0.79208 - acc : 0.94 - val loss : 0.80291 - val acc : 0.96\n",
      "[Epoch 1/2] [Batch 124/125] [Loss: 0.800000 (0.802734)] epoch 1 - loss : 0.80273 - acc : 0.97 - val loss : 0.80499 - val acc : 0.96\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "os.makedirs('./saved_models', exist_ok=True)\n",
    "\n",
    "N_EPOCHS = 2\n",
    "N_DATA = len(train_dataset)\n",
    "N_TEST = len(test_dataset)\n",
    "\n",
    "plot_losses = []\n",
    "scheduler_counter = 0\n",
    "prediction = []\n",
    "# preds = []\n",
    "for epoch in range(N_EPOCHS):\n",
    "  # training\n",
    "  model.train()\n",
    "  loss_list = []\n",
    "  acc_list = []\n",
    "  # epochPreds = []\n",
    "  for batch_i, (x, y, z) in enumerate(train_dataloader):\n",
    "    # batchPreds = []\n",
    "    for j in range(len(x)):\n",
    "\n",
    "      pred_mask = model(x.to(device)[j:j+1])\n",
    "      mask = torch.argmax(pred_mask, axis=1).cpu().detach().numpy()[0]\n",
    "      # print(\"-----------------------------------\")\n",
    "      # print(z[j])\n",
    "      # print(\"-----------------------------------\")\n",
    "      # imageIndex = getImageIndex(epochPreds,z[j][38:64])\n",
    "      # imageSize = getImageSize(trainImageSizes,z[j][38:64])\n",
    "      # cordinates = (z[j][65:-4]).split(\"_\")\n",
    "      # im = np.moveaxis(x.to(device)[j].cpu().detach().numpy(), 0, -1).copy()*255\n",
    "      # im = im.astype(int)\n",
    "\n",
    "      # gt_mask = y[j]\n",
    "      # gt_mask = gt_mask.cpu().detach().numpy()\n",
    "      # if imageIndex!=-1:\n",
    "      #   (epochPreds[imageIndex][1]).append([cordinates,im,gt_mask,mask])\n",
    "      # else:\n",
    "      #   epochPreds.append([z[j][38:64],[[cordinates,im,gt_mask,mask]],imageSize])\n",
    "\n",
    "      loss = criterion(pred_mask, y.to(device))\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      loss_list.append(loss.cpu().detach().numpy())\n",
    "      acc_list.append(acc(y,pred_mask).numpy())\n",
    "\n",
    "      sys.stdout.write(\n",
    "          \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f (%f)]\"\n",
    "          % (\n",
    "              epoch,\n",
    "              N_EPOCHS,\n",
    "              batch_i,\n",
    "              len(train_dataloader),\n",
    "              loss.cpu().detach().numpy(),\n",
    "              np.mean(loss_list),\n",
    "          )\n",
    "      )\n",
    "    # epochPreds.append(batchPreds)\n",
    "  scheduler_counter += 1\n",
    "  # testing\n",
    "  model.eval()\n",
    "  val_loss_list = []\n",
    "  val_acc_list = []\n",
    "  for batch_i, (x, y, z) in enumerate(test_dataloader):\n",
    "      with torch.no_grad():    \n",
    "          pred_mask = model(x.to(device))\n",
    "      val_loss = criterion(pred_mask, y.to(device))\n",
    "      val_loss_list.append(val_loss.cpu().detach().numpy())\n",
    "      val_acc_list.append(acc(y,pred_mask).numpy())\n",
    "    \n",
    "  print(' epoch {} - loss : {:.5f} - acc : {:.2f} - val loss : {:.5f} - val acc : {:.2f}'.format(epoch, \n",
    "                                                                                                 np.mean(loss_list), \n",
    "                                                                                                 np.mean(acc_list), \n",
    "                                                                                                 np.mean(val_loss_list),\n",
    "                                                                                                 np.mean(val_acc_list)))\n",
    "  plot_losses.append([epoch, np.mean(loss_list), np.mean(val_loss_list)])\n",
    "  #  print(\"prediction \", prediction.size(), prediction.data[0])\n",
    "  compare_loss = np.mean(val_loss_list)\n",
    "  is_best = compare_loss < min_loss\n",
    "  if is_best == True:\n",
    "    # preds = epochPreds\n",
    "    scheduler_counter = 0\n",
    "    min_loss = min(compare_loss, min_loss)\n",
    "  torch.save(model.state_dict(), './saved_models/unet_epoch_{}_{:.5f}.pt'.format(epoch,np.mean(val_loss_list)))\n",
    "  \n",
    "  if scheduler_counter > 5:\n",
    "    lr_scheduler.step()\n",
    "    print(f\"lowering learning rate to {optimizer.param_groups[0]['lr']}\")\n",
    "    scheduler_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhL0W9i2IfkJ"
   },
   "outputs": [],
   "source": [
    "# os.makedirs('./saved_models', exist_ok=True)\n",
    "\n",
    "# N_EPOCHS = 20\n",
    "# N_DATA = len(train_dataset)\n",
    "# N_TEST = len(test_dataset)\n",
    "\n",
    "# plot_losses = []\n",
    "# scheduler_counter = 0\n",
    "# prediction = []\n",
    "# preds = []\n",
    "# for epoch in range(N_EPOCHS):\n",
    "#   # training\n",
    "#   model.train()\n",
    "#   loss_list = []\n",
    "#   acc_list = []\n",
    "#   epochPreds = []\n",
    "#   for batch_i, (x, y, z) in enumerate(train_dataloader):\n",
    "#     # batchPreds = []\n",
    "#     for j in range(len(x)):\n",
    "\n",
    "#       pred_mask = model(x.to(device)[j:j+1])\n",
    "#       mask = torch.argmax(pred_mask, axis=1).cpu().detach().numpy()[0]\n",
    "#       # print(\"-----------------------------------\")\n",
    "#       # print(z[j])\n",
    "#       # print(\"-----------------------------------\")\n",
    "#       imageIndex = getImageIndex(epochPreds,z[j][38:64])\n",
    "#       imageSize = getImageSize(trainImageSizes,z[j][38:64])\n",
    "#       cordinates = (z[j][65:-4]).split(\"_\")\n",
    "#       im = np.moveaxis(x.to(device)[j].cpu().detach().numpy(), 0, -1).copy()*255\n",
    "#       im = im.astype(int)\n",
    "\n",
    "#       gt_mask = y[j]\n",
    "#       gt_mask = gt_mask.cpu().detach().numpy()\n",
    "#       if imageIndex!=-1:\n",
    "#         (epochPreds[imageIndex][1]).append([cordinates,im,gt_mask,mask])\n",
    "#       else:\n",
    "#         epochPreds.append([z[j][38:64],[[cordinates,im,gt_mask,mask]],imageSize])\n",
    "\n",
    "#       loss = criterion(pred_mask, y.to(device))\n",
    "\n",
    "#       optimizer.zero_grad()\n",
    "#       optimizer.step()\n",
    "#       loss_list.append(loss.cpu().detach().numpy())\n",
    "\n",
    "#       sys.stdout.write(\n",
    "#           \"\\r[Epoch %d/%d] [Batch %d/%d] [Loss: %f (%f)]\"\n",
    "#           % (\n",
    "#               epoch,\n",
    "#               N_EPOCHS,\n",
    "#               batch_i,\n",
    "#               len(train_dataloader),\n",
    "#               loss.cpu().detach().numpy(),\n",
    "#               np.mean(loss_list),\n",
    "#           )\n",
    "#       )\n",
    "#     # epochPreds.append(batchPreds)\n",
    "#   scheduler_counter += 1\n",
    "#   # testing\n",
    "#   model.eval()\n",
    "#   val_loss_list = []\n",
    "#   val_acc_list = []\n",
    "#   for batch_i, (x, y, z) in enumerate(test_dataloader):\n",
    "#       with torch.no_grad():    \n",
    "#           pred_mask = model(x.to(device))\n",
    "#       val_loss = criterion(pred_mask, y.to(device))\n",
    "#       val_loss_list.append(val_loss.cpu().detach().numpy())\n",
    "#       val_acc_list.append(acc(y,pred_mask).numpy())\n",
    "    \n",
    "#   print(' epoch {} - loss : {:.5f} - acc : {:.2f} - val loss : {:.5f} - val acc : {:.2f}'.format(epoch, \n",
    "#                                                                                                  np.mean(loss_list), \n",
    "#                                                                                                  np.mean(acc_list), \n",
    "#                                                                                                  np.mean(val_acc_list)))\n",
    "#   plot_losses.append([epoch, np.mean(loss_list), np.mean(val_loss_list)])\n",
    "#   #  print(\"prediction \", prediction.size(), prediction.data[0])\n",
    "#   compare_loss = np.mean(val_loss_list)\n",
    "#   is_best = compare_loss < min_loss\n",
    "#   if is_best == True:\n",
    "#     preds = epochPreds\n",
    "#     scheduler_counter = 0\n",
    "#     min_loss = min(compare_loss, min_loss)\n",
    "#   torch.save(model.state_dict(), './saved_models/unet_epoch_{}_{:.5f}.pt'.format(epoch,np.mean(val_loss_list)))\n",
    "  \n",
    "#   if scheduler_counter > 5:\n",
    "#     lr_scheduler.step()\n",
    "#     print(f\"lowering learning rate to {optimizer.param_groups[0]['lr']}\")\n",
    "#     scheduler_counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# model.load_state_dict(torch.load('/content/saved_models/unet_epoch_0_0.80383.pt'))"
   ],
   "metadata": {
    "id": "QJaWiy1EE4Dw"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# import classifier model\n",
    "# url = \"https://drive.google.com/drive/u/0/folders/1I1vxbZxmgR3OOPE7e-odfpcuCmJElICl\"\n",
    "# gdown.download_folder(url, quiet=True, use_cookies=False, remaining_ok=True)\n",
    "\n",
    "# model.load_state_dict(torch.load('/content/saved_models/unet_epoch_18_0.89815.pt'))\n",
    "model.load_state_dict(torch.load('/content/MIC Classifier/segmentation_model.pt'))\n",
    "model.eval()"
   ],
   "metadata": {
    "id": "l2ASx0bPJ2ed"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "predictions = []\n",
    "# model.eval()\n",
    "m = nn.Softmax(dim=1)\n",
    "# print(testImageSizes)\n",
    "for batch_i, (x, y,z) in enumerate(train_dataloader):\n",
    "    for j in range(len(x)):\n",
    "      result = model(x.to(device)[j:j+1])\n",
    "      # mask = torch.argmax(result, axis=1).cpu().detach().numpy()[0]\n",
    "      mask1 = m(torch.from_numpy(result.cpu().detach().numpy()[0]))\n",
    "      mask = torch.argmax(mask1, axis=1).cpu().detach().numpy()\n",
    "      # mask = result.cpu().detach().numpy()[0]\n",
    "      imageIndex = getImageIndex(predictions,z[j][38:64])\n",
    "      # print(\"list \",z[j])\n",
    "      imageSize = getImageSize(trainImageSizes,z[j][38:64])\n",
    "      cordinates = (z[j][65:-4]).split(\"_\")\n",
    "      \n",
    "      im = np.moveaxis(x.to(device)[j].cpu().detach().numpy(), 0, -1).copy()*255\n",
    "      im = im.astype(int)\n",
    "      \n",
    "      gt_mask = y[j]\n",
    "      gt_mask = gt_mask.cpu().detach().numpy()\n",
    "      if imageIndex!=-1:\n",
    "        (predictions[imageIndex][1]).append([cordinates,im,gt_mask,mask])\n",
    "      else:\n",
    "        predictions.append([z[j][38:64],[[cordinates,im,gt_mask,mask]],imageSize])\n",
    "\n",
    "        # imageIndex = getImageIndex(epochPreds,z[j][38:64])\n",
    "#       imageSize = getImageSize(trainImageSizes,z[j][38:64])\n",
    "#       cordinates = (z[j][65:-4]).split(\"_\")\n",
    "#       im = np.moveaxis(x.to(device)[j].cpu().detach().numpy(), 0, -1).copy()*255\n",
    "#       im = im.astype(int)\n",
    "\n",
    "#       gt_mask = y[j]\n",
    "#       gt_mask = gt_mask.cpu().detach().numpy()\n",
    "#       if imageIndex!=-1:\n",
    "#         (epochPreds[imageIndex][1]).append([cordinates,im,gt_mask,mask])\n",
    "#       else:\n",
    "#         epochPreds.append([z[j][38:64],[[cordinates,im,gt_mask,mask]],imageSize])"
   ],
   "metadata": {
    "id": "l2o5QWX0GQOJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "a = torch.randn(4, 4)\n",
    "a"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cu5XR3jXq8qf",
    "outputId": "d35cf9b1-63f5-4ecd-cc5b-da359126ed79"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.4036, -1.5211,  0.3832, -0.4665],\n",
       "        [ 0.1664,  2.1122, -0.7550, -0.2734],\n",
       "        [ 1.2436,  0.8844,  1.9544,  0.5287],\n",
       "        [-0.7093, -0.7989,  1.2137, -0.1271]])"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "m = nn.Softmax(dim=1)\n",
    "output = m(a)\n",
    "output"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KrZarEoorAMf",
    "outputId": "9625829a-2803-4999-a257-5d0cef81b89f"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.3930, 0.0573, 0.3850, 0.1646],\n",
       "        [0.1106, 0.7741, 0.0440, 0.0712],\n",
       "        [0.2368, 0.1653, 0.4820, 0.1158],\n",
       "        [0.0948, 0.0867, 0.6488, 0.1697]])"
      ]
     },
     "metadata": {},
     "execution_count": 39
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "for e in predictions:\n",
    "  print(e[1][0][3])\n",
    "  break"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "srLZaEkRmjz7",
    "outputId": "6980ff27-6f7f-4dac-b305-b0115f0d94a5"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "yoeeJbVtWcS3",
    "outputId": "3d15fdac-a81b-41c7-9a9b-cf36ade2cfe4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'3_4'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = \"./TrainingData/TrainingPatches/images/PSP_007704_1765_RED.browse_3_4.jpg\"\n",
    "l[65:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-zBO0EOvMZ9_"
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def constructImageRow(rowPatches):\n",
    "  sortedPatches = sorted(rowPatches, key=itemgetter(0))\n",
    "  rowImge =  torch.from_numpy(sortedPatches[0][1])\n",
    "  for pt in sortedPatches[1:]:\n",
    "    rowImge = torch.cat(((rowImge),torch.from_numpy(pt[1])), 1)\n",
    "  return rowImge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Elen9omKMb0P"
   },
   "outputs": [],
   "source": [
    "# from operator import itemgetter\n",
    "\n",
    "# def reconstructImage(ImagePatches,type):\n",
    "#   variable =True\n",
    "#   current_row = 0\n",
    "#   rows =[]\n",
    "#   imageSize = ImagePatches[2]\n",
    "#   # print(\"size \",imageSize  )\n",
    "#   imageWidth = imageSize[0]\n",
    "#   imageHeight = imageSize[1]\n",
    "#   predictionPatches = sorted(ImagePatches[1], key=int(itemgetter(0)))\n",
    "#   for i in predictionPatches:\n",
    "#     print(i[0])\n",
    "#   for n in range(0,len(predictionPatches)):\n",
    "#     y_cor = int(predictionPatches[n][0][0])\n",
    "#     x_cor = int(predictionPatches[n][0][1])\n",
    "    \n",
    "#     if (current_row==y_cor and x_cor==0):\n",
    "#       rowPatches = [[x_cor,predictionPatches[n][type]]]\n",
    "#       # rowImage = torch.from_numpy(predictionPatches[n][type])\n",
    "#     elif (current_row==y_cor and x_cor!=0):\n",
    "#       rowPatches.append([x_cor,predictionPatches[n][type]])\n",
    "#       # rowImage = torch.cat(((rowImage),torch.from_numpy(predictionPatches[n][type])), 1)\n",
    "#     else:\n",
    "#       # numPatchesInRow = int(predictionPatches[n-1][0][1])+1\n",
    "#       numPatchesInRow = len(rowPatches)+1\n",
    "#       newRow = constructImageRow(rowPatches)\n",
    "#       rows.append([current_row,newRow])\n",
    "#       rowPatches = [[x_cor,predictionPatches[n][type]]]\n",
    "#       # rowImage = torch.from_numpy(predictionPatches[n][type])\n",
    "#       current_row = y_cor\n",
    "#     if(n+1==len(predictionPatches)):\n",
    "#       newRow = constructImageRow(rowPatches)\n",
    "#       rows.append([y_cor,newRow])\n",
    "#       variable = False\n",
    "\n",
    "  \n",
    "#   rows = sorted(rows, key=itemgetter(0))\n",
    "  \n",
    "#   fullImage = rows[0][1]\n",
    "#   # print(\"y_cor\", rows[0][0])\n",
    "#   # plt.imshow(rows[0][1])\n",
    "#   # plt.show()\n",
    "#   for row in rows[1:]:\n",
    "#     fullImage = torch.cat(((fullImage),(row[1])), 0)\n",
    "#   croppedImage = fullImage[0:imageHeight,0:imageWidth]\n",
    "\n",
    "#   return croppedImage\n",
    "\n",
    "# # ImageOut = reconstructImage(predictions[2])\n",
    "# # plt.figure(figsize=(15,15))   \n",
    "# # plt.imshow(ImageOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4PFGlf5EHtr"
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "def reconstructImage(ImagePatches,type):\n",
    "  variable =True\n",
    "  current_row = 0\n",
    "  rows =[]\n",
    "  imageSize = ImagePatches[2]\n",
    "  # print(\"size \",imageSize  )\n",
    "  imageWidth = imageSize[0]\n",
    "  imageHeight = imageSize[1]\n",
    "  # predictionPatches = ImagePatches[1]\n",
    "  predictionPatches = sorted(ImagePatches[1], key=itemgetter(0))\n",
    "  for n in range(0,len(predictionPatches)):\n",
    "    y_cor = int(predictionPatches[n][0][0])\n",
    "    x_cor = int(predictionPatches[n][0][1])\n",
    "    \n",
    "    if (current_row==y_cor and x_cor==0):\n",
    "      rowPatches = [[x_cor,predictionPatches[n][type]]]\n",
    "      # rowImage = torch.from_numpy(predictionPatches[n][type])\n",
    "    elif (current_row==y_cor and x_cor!=0):\n",
    "      rowPatches.append([x_cor,predictionPatches[n][type]])\n",
    "      # rowImage = torch.cat(((rowImage),torch.from_numpy(predictionPatches[n][type])), 1)\n",
    "    else:\n",
    "      # numPatchesInRow = int(predictionPatches[n-1][0][1])+1\n",
    "      numPatchesInRow = len(rowPatches)+1\n",
    "      newRow = constructImageRow(rowPatches)\n",
    "      rows.append([current_row,newRow])\n",
    "      rowPatches = [[x_cor,predictionPatches[n][type]]]\n",
    "      # rowImage = torch.from_numpy(predictionPatches[n][type])\n",
    "      current_row = y_cor\n",
    "    if(n+1==len(predictionPatches)):\n",
    "      newRow = constructImageRow(rowPatches)\n",
    "      rows.append([y_cor,newRow])\n",
    "      variable = False\n",
    "\n",
    "  \n",
    "  rows = sorted(rows, key=itemgetter(0))\n",
    "  \n",
    "  fullImage = rows[0][1]\n",
    "  # print(\"y_cor\", rows[0][0])\n",
    "  # plt.imshow(rows[0][1])\n",
    "  # plt.show()\n",
    "  for row in rows[1:]:\n",
    "    fullImage = torch.cat(((fullImage),(row[1])), 0)\n",
    "  croppedImage = fullImage[0:imageHeight,0:imageWidth]\n",
    "\n",
    "  return croppedImage\n",
    "\n",
    "# ImageOut = reconstructImage(predictions[2])\n",
    "# plt.figure(figsize=(15,15))   \n",
    "# plt.imshow(ImageOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AKGoDM4_Omdr",
    "outputId": "64f4f015-9bae-4a8d-d536-f9c639a5f4e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "# print(preds[0][0][0][1][0][1])\n",
    "print(len(preds[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5xDbg6K75MDz",
    "outputId": "3b8cb766-362e-4f98-c668-5424827f0810"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "for element in preds[0]:\n",
    "  print(len(element[1]))\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MtHLXkfAgd2g",
    "outputId": "a582e71e-b449-4180-b8b0-f776cd97315c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1519, 3947]\n"
     ]
    }
   ],
   "source": [
    "for element in preds[0]:\n",
    "  print(element[2])\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "id": "cnHh_BMKMnid",
    "outputId": "c11bc722-cbcb-4a98-f70d-afa4ddc92b0f"
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-38-cead014aa7da>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m   \u001B[0mimage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreconstructImage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0melement\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m   \u001B[0mannotatedMask\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreconstructImage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0melement\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 9\u001B[0;31m   \u001B[0mpredictionMask\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreconstructImage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0melement\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m   \u001B[0;31m# f1_scored = f1_score(annotatedMask,predictionMask, average='weighted',zero_division=1)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m   \u001B[0;31m# jaccard = jaccard_score(annotatedMask,predictionMask,average='weighted',zero_division=1)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-37-a3db71d41aee>\u001B[0m in \u001B[0;36mreconstructImage\u001B[0;34m(ImagePatches, type)\u001B[0m\n\u001B[1;32m     24\u001B[0m       \u001B[0;31m# numPatchesInRow = int(predictionPatches[n-1][0][1])+1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m       \u001B[0mnumPatchesInRow\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrowPatches\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m       \u001B[0mnewRow\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconstructImageRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrowPatches\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m       \u001B[0mrows\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mcurrent_row\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mnewRow\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m       \u001B[0mrowPatches\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mx_cor\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mpredictionPatches\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mn\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-36-3b99f56407b0>\u001B[0m in \u001B[0;36mconstructImageRow\u001B[0;34m(rowPatches)\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mconstructImageRow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrowPatches\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m   \u001B[0msortedPatches\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msorted\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrowPatches\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mitemgetter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m   \u001B[0mrowImge\u001B[0m \u001B[0;34m=\u001B[0m  \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msortedPatches\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m   \u001B[0;32mfor\u001B[0m \u001B[0mpt\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msortedPatches\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m     \u001B[0mrowImge\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrowImge\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mtorch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfrom_numpy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpt\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mTypeError\u001B[0m: expected np.ndarray (got Tensor)"
     ]
    }
   ],
   "source": [
    "from skimage import io, morphology\n",
    "from sklearn.metrics import f1_score   \n",
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "# for element in preds[0]:\n",
    "for element in predictions:\n",
    "  image = reconstructImage(element,1)\n",
    "  annotatedMask = reconstructImage(element,2)\n",
    "  predictionMask = reconstructImage(element,3)\n",
    "  # f1_scored = f1_score(annotatedMask,predictionMask, average='weighted',zero_division=1)\n",
    "  # jaccard = jaccard_score(annotatedMask,predictionMask,average='weighted',zero_division=1)\n",
    "  # print(\"F1 Score = \",f1_scored)\n",
    "  # print(\"Jaccaard Score = \",jaccard)\n",
    "  plt.figure(figsize=(14,14))\n",
    "  plt.subplot(1,3,1)\n",
    "  plt.imshow(image)\n",
    "  annotated= annotatedMask.cpu().detach().numpy()\n",
    "  annotated[annotated<0.5] = 255\n",
    "  annotated[annotated==1] = 0\n",
    "  plt.subplot(1,3,2)\n",
    "  plt.imshow(annotatedMask ,cmap = 'gray')\n",
    "  \n",
    "  \n",
    "  numpyPred = predictionMask.cpu().detach().numpy()\n",
    "  numpyPred[numpyPred<0.5] = 255\n",
    "  numpyPred[numpyPred==1] = 0\n",
    "  plt.subplot(1,3,3)\n",
    "  plt.imshow(numpyPred, cmap = 'gray')\n",
    "  \n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DLIJNwdVE6n7"
   },
   "outputs": [],
   "source": [
    "# plot loss\n",
    "plot_losses = np.array(plot_losses)\n",
    "plt.plot(plot_losses[:,0], plot_losses[:,1], color='b', linewidth=4)\n",
    "plt.plot(plot_losses[:,0], plot_losses[:,2], color='r', linewidth=4)\n",
    "plt.title('Dice Loss', fontsize=20)\n",
    "plt.xlabel('epoch',fontsize=20)\n",
    "plt.ylabel('loss',fontsize=20)\n",
    "plt.grid()\n",
    "plt.legend(['training', 'validation']) # using a named size\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-6_gkWTSE9SN"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('/content/saved_models/unet_epoch_24_0.78482.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qM_k7YYmHh8J"
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "model.eval()\n",
    "# print(testImageSizes)\n",
    "for batch_i, (x, y,z) in enumerate(test_dataloader):\n",
    "    for j in range(len(x)):\n",
    "      result = model(x.to(device)[j:j+1])\n",
    "      mask = torch.argmax(result, axis=1).cpu().detach().numpy()[0]\n",
    "      imageIndex = getImageIndex(predictions,z[j][36:62])\n",
    "      # print(\"list \",z[j])\n",
    "      imageSize = getImageSize(testImageSizes,z[j][36:62])\n",
    "      cordinates = (z[j][63:-4]).split(\"_\")\n",
    "      \n",
    "      im = np.moveaxis(x.to(device)[j].cpu().detach().numpy(), 0, -1).copy()*255\n",
    "      im = im.astype(int)\n",
    "      \n",
    "      gt_mask = y[j]\n",
    "      gt_mask = gt_mask.cpu().detach().numpy()\n",
    "      if imageIndex!=-1:\n",
    "        (predictions[imageIndex][1]).append([cordinates,im,gt_mask,mask])\n",
    "      else:\n",
    "        predictions.append([z[j][36:-8],[[cordinates,im,gt_mask,mask]],imageSize])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
